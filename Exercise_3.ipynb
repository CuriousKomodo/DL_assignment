{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning - Question 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-fc73748e7dbf>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-fc73748e7dbf>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    W1 =\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#has one hidden layer of size 30\n",
    "#wtf is phi and sigma???\n",
    "#How do we choose phi and sigma?\n",
    "#How do we initialize? \n",
    "\n",
    "#Random, out of N(0,1)? Do we need normalization\n",
    "def intialize(x,h=30):\n",
    "    m,n = x.shape()\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
    "    W1 = np.random.randn(h, m) * 0.01 #rough\n",
    "    b1 = np.zeros(shape=(h, 1))\n",
    "    W2 = np.random.randn(m, h) * 0.01 #rough\n",
    "    b2 = np.zeros(shape=(m, 1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(W1.shape == (h, m))\n",
    "    assert(b1.shape == (h, 1))\n",
    "    assert(W2.shape == (m, h))\n",
    "    assert(b2.shape == (m, 1))\n",
    "    \n",
    "    param = {\"W1\": W1,\n",
    "             \"b1\": b1,\n",
    "             \"W2\": W2,\n",
    "             \"b2\": b2}\n",
    "    \n",
    "    return param\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(exp(-x)) #have no idea what activation function it is ... for now its sigmoid, may need to choose using CV \n",
    "\n",
    "def phi(x,phi_choice):\n",
    "    if phi_choice =='tanh':\n",
    "        return np.tanh(x) #again, phi can be any function. maybe need to choose using cross validation OMG\n",
    "    elif phi_choice =='relu':\n",
    "        return max(x,0)\n",
    "    elif phi_choice =='sigmoid':\n",
    "        return sigmoid(x)\n",
    "\n",
    "def dphi(x):\n",
    "    if phi_choice =='tanh':\n",
    "        return 1.0-np.tanh(x)**2\n",
    "    elif phi_choice =='relu':\n",
    "        return (1 if x>0 else 0)\n",
    "    elif phi_choice =='sigmoid':\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "    \n",
    "def forward_propagation(param, x, phi_choice):\n",
    "    tree = {}\n",
    "    W1 = param['W1']\n",
    "    b1 = param['b1']\n",
    "    W2 = param['W2']\n",
    "    b2 = param['b2']\n",
    "    \n",
    "    tree['x'] = x\n",
    "    tree['z1'] =  np.matmul(W1,x)+b1\n",
    "    tree['h'] = phi(tree['z1'],phi_choice)\n",
    "    tree['z2'] = np.matmul(W2,tree['h'])+b2\n",
    "    tree['x_hat'] = sigmoid(tree['z2'])\n",
    "    \n",
    "    return tree\n",
    "\n",
    "def square_loss(x,y):\n",
    "    m = x.shape[0]\n",
    "    loss = (1.0/m)*np.dot((x-y).T,(x-y))\n",
    "    assert(loss.shape == ())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def cross_entropy_loss(x,y):\n",
    "    loss = -x*np.log(y) - (1-x)*np.log(1-y)\n",
    "    assert(loss.shape == ())\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def backpropagation_MSE(tree,phi_choice):\n",
    "    #depends on the choice of phi and sigma function?\n",
    "    grads = {}\n",
    "    dW2 = tree\n",
    "    db2 = \n",
    "    dW1 = \n",
    "    db1 = \n",
    "    \n",
    "    return grads\n",
    "\n",
    "def backpropagation_CE(tree,phi_choice):\n",
    "    #depends on the choice of phi and sigma function?\n",
    "    grads = {}\n",
    "    dW2 = \n",
    "    db2 = \n",
    "    dW1 = \n",
    "    db1 = \n",
    "    \n",
    "    return grads\n",
    "\n",
    "\n",
    "def initialize_adam_optimizer(param):\n",
    "    m = {}\n",
    "    v = {}\n",
    "      \n",
    "    for l in range(L):\n",
    "        m[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        m[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "\n",
    "        v[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        v[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "    \n",
    "    return m,v\n",
    "\n",
    "#But what are the values of b1, b2, learning_rate and gamma? \n",
    "def update_adam_optimizer(param,grads,m,v,learning_rate,b1=0.9,b2=0.999,gamma=10**(-8)):\n",
    "    L = len(param) // 2   #number of parameters\n",
    "    \n",
    "    m_hat = {}\n",
    "    v_hat = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        m[\"dW\" + str(l + 1)] = beta1 * m[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n",
    "        m[\"db\" + str(l + 1)] = beta1 * m[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n",
    "        v[\"dW\" + str(l + 1)] = beta2 * v[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n",
    "        v[\"db\" + str(l + 1)] = beta2 * v[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n",
    "        \n",
    "        m_hat[\"dW\" + str(l + 1)] = m[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
    "        m_hat[\"db\" + str(l + 1)] = m[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
    "        \n",
    "        v_hat[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
    "        v_hat[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
    "        \n",
    "        param[\"W\" + str(l + 1)] = param[\"W\" + str(l + 1)] - learning_rate * m_hat[\"dW\" + str(l + 1)] / np.sqrt(v_hat[\"dW\" + str(l + 1)] + gamma)\n",
    "        param[\"b\" + str(l + 1)] = param[\"b\" + str(l + 1)] - learning_rate * m_hat[\"db\" + str(l + 1)] / np.sqrt(v_hat[\"db\" + str(l + 1)] + gamma)\n",
    "        \n",
    "    return m,v,param\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
